{"cells":[{"cell_type":"markdown","metadata":{"id":"MtloB3Y39lTq"},"source":["# Dependencies"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n","import numpy as np\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","import random\n","import gc\n","from pathlib import Path\n","from sentence_transformers import SentenceTransformer, util\n"]},{"cell_type":"markdown","metadata":{"id":"IR6vI0ykuc15"},"source":["# Constants"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["SEED = 2021 \n","BINS = [float('inf'), 1.5, 1, 0.5, 0, -0.5, -1, -1.5, -2, -2.5, -3, -3.5, float('-inf')] # map the raw score to readability level from 1 to 12(easy to hard)\n","MAX_LENGTH = 256 # the maximum length of the texts feed to the model\n","CORPORA_LIST = ['simplewiki','wiki','bookcorpus']\n","\n","TRAIN_FILE_ORIG=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"train.csv\")\n","TRAIN_FILE_SPLIT=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"train_split.csv\")\n","VAL_FILE_SPLIT=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"val_split.csv\")\n","TRAIN_FILE_EXTENDED=os.path.join(Path(os.getcwd()).parent,'data','training','extended','train_augmented.csv')\n","\n","BASELINE_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"base\")\n","PRETRAIN_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"pretrain\")\n","FINETUNE_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"finetune\")\n","FINAL_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"final\")\n","\n","EMBEDDINGS_DIR=os.path.join(Path(os.getcwd()).parent,'data','embeddings')\n","EXTENDED_DATA_DIR=os.path.join(Path(os.getcwd()).parent,'data','training','extended')"]},{"cell_type":"markdown","metadata":{"id":"fxlIQpd1un1x"},"source":["# Utiliy Functions"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"CRW0J5Y462Sb"},"outputs":[],"source":["def seed_everything(seed=2021):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything(seed=2021)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"VQG27hNjrL5l"},"outputs":[],"source":["def search_similar_passages(original_texts, external_texts_list, top_k):\n","  model = SentenceTransformer('paraphrase-TinyBERT-L6-v2')\n","  external_embeddings = []\n","  sentences = []\n","  for dataset in external_texts_list:\n","    embeddings_dir = os.path.join(EMBEDDINGS_DIR, 'encoded-'+ dataset + '.pt')\n","    if os.path.isfile(embeddings_dir):\n","      encoded = torch.load(embeddings_dir)\n","      external_embeddings.extend(encoded)\n","    else:\n","      raise FileNotFoundError(f'{dataset} embeddings could not be found.')\n","    texts_dir = os.path.join(EXTENDED_DATA_DIR,dataset+'.csv')\n","    if os.path.isfile(texts_dir):\n","      sents = pd.read_csv(texts_dir)\n","      sents = sents.text.values\n","      sentences.extend(sents)\n","    else:\n","      raise FileNotFoundError(f'{dataset} texts could not be found.')\n","    assert len(external_embeddings) == len(sentences)\n","  original_embeddings = model.encode(original_texts, convert_to_tensor=True)\n","  #for each original embedding, find a list of top_k entries(each entry is a pair of corpus_id(index in external_embeddings in our case) and similar score) sorted in descending similarity score.\n","  #We call one list of this kind is a hit. If there are A original-embeddings, then the \"hits\" below consists of A hits. \n","  #totally, A*top_K similar embeddings will be returned after the function \"util.semantic_search\" executed.\n","  hits = util.semantic_search(original_embeddings, external_embeddings, top_k=top_k, corpus_chunk_size=80000)\n","  selected = []\n","  #retrive the passages from the list of 'sentences' based on the corpus_id from hits\n","  for hit in hits:\n","    sents = [sentences[h['corpus_id']] for h in hit]\n","    selected.append(sents)\n","  return selected\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Lagv6uH3vo62"},"outputs":[],"source":["#zip the sentence, score(outcome of baseline model) and standard deviation \n","def zip_hits_scores(hits, scores, stdev):\n","  zipped = []\n","  for idx, hit in enumerate(hits):\n","    current = [(h, scores[idx], stdev[idx]) for h in hit]\n","    zipped.extend(current)\n","  return zipped"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"1RCtumYzvrzz"},"outputs":[],"source":["#filter out those texts with score(outcome of baseline modelï¼‰out of range of one std centered with it's corresponding original text's ground truth target \n","def filter_on_stdev(sentences, predictions, targets, stdev):\n","  pred_filtered = []\n","  sents_filtered = []\n","  for idx, pred in enumerate(predictions):\n","    if abs(pred-targets[idx]) < stdev[idx]:\n","      pred_filtered.append(pred)\n","      sents_filtered.append(sentences[idx])  \n","  return sents_filtered, pred_filtered"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"gw_ED7B7x_F1"},"outputs":[],"source":["#split the date into chunksize batch\n","def chunks(lst, chunksize):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), chunksize):\n","        yield lst[i:i + chunksize]"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"4wzHLOB9w5-X"},"outputs":[],"source":["#make prediction \n","def predict(model_name_or_dir, data):\n","  device = \"cuda:0\"\n","  config = AutoConfig.from_pretrained(model_name_or_dir, num_labels=1)\n","  tokenizer=AutoTokenizer.from_pretrained(config._name_or_path)\n","  model = AutoModelForSequenceClassification.from_pretrained(model_name_or_dir, config=config)\n","  model.to(device)\n","  model.eval()\n","  y_pred = []\n","  batches = chunks(data, 32)\n","  for batch in tqdm(batches):\n","    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n","    input_ids = inputs['input_ids'].to(device)\n","    attention = inputs['attention_mask'].to(device)\n","    inputs = {\n","        'input_ids': input_ids,\n","        'attention_mask': attention\n","    }\n","    with torch.no_grad():        \n","          outputs = model(**inputs)\n","    logits = outputs[0].detach().cpu().numpy().squeeze().tolist()    \n","    y_pred.extend(logits)\n","  del model\n","  return y_pred"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"qgF-nPh6w5AD"},"outputs":[],"source":["#The main function to generate augmented dataset by searching similar texts in external dataset, and pseudo-labeling them with baseline model \n","def generate_augmented_dataset(original_dir, external_dataset_list, model_dir, out_dir, top_k=10):\n","  torch.cuda.empty_cache()\n","  print(\"Load original dataset\")\n","  original_train_df = pd.read_csv(original_dir)\n","  queries = [str(t) for t in original_train_df.excerpt.values]\n","  scores = [float(t) for t in original_train_df.target.values]\n","  stdev = [float(t) for t in original_train_df.standard_error.values]\n","  print('Start to search similar passages')\n","  #for each passage in the original corpus, select top_k similar passages in the external corpus\n","  hits = search_similar_passages(queries, external_dataset_list, top_k)\n","  zipped = zip_hits_scores(hits, scores, stdev)\n","  sentences = [t[0] for t in zipped]\n","  scores = [t[1] for t in zipped]\n","  stdev = [t[2] for t in zipped]\n","  torch.cuda.empty_cache()\n","  print('Predict target of the selected passages')\n","  predictions = predict(model_dir, sentences)\n","  print('Remove passages which predicted target is outside of the stdev(sandard deviation)')\n","  sents_filtered, preds_filtered = filter_on_stdev(sentences, predictions, scores, stdev)\n","  augmented_df = pd.DataFrame.from_dict({'excerpt': sents_filtered, 'target': preds_filtered})\n","  augmented_df.to_csv(out_dir)\n","  print(f'Selected passages are saved')\n","  torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"ZFTyzUGqyJxw"},"source":["# Pseudo label the external texts with baseline model to generate augmented training dataset"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210264,"status":"ok","timestamp":1628345477525,"user":{"displayName":"Mathis Lucka","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhthkjGUPZoLLxPuKrqVqOhzkL6AX8O9OT0agPB=s64","userId":"17007389841511802481"},"user_tz":-120},"id":"KM2V4ydT3EWY","outputId":"16c01813-94b9-46b9-a299-0344729fb4c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load original dataset\n","Start to search similar passages\n","Predict target of the selected passages\n"]},{"name":"stderr","output_type":"stream","text":["886it [04:51,  3.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Remove passages which predicted target is outside of the stdev(sandard deviation)\n","Selected passages are saved\n"]}],"source":["generate_augmented_dataset(TRAIN_FILE_ORIG, CORPORA_LIST,BASELINE_MODEL_DIR, TRAIN_FILE_EXTENDED)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10097 entries, 0 to 10096\n","Data columns (total 3 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Unnamed: 0  10097 non-null  int64  \n"," 1   excerpt     10097 non-null  object \n"," 2   target      10097 non-null  float64\n","dtypes: float64(1), int64(1), object(1)\n","memory usage: 236.8+ KB\n"]}],"source":["augmented_df=pd.read_csv(TRAIN_FILE_EXTENDED)\n","augmented_df.info()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNocH8Qu3bl0DtfHJwPJ1j3","collapsed_sections":[],"machine_shape":"hm","name":"03_clrp_external_data_labeling.ipynb","provenance":[]},"interpreter":{"hash":"1bb66c20f33883921ac4c6fb43838a8dacd3a8d29c92ea31e661ca910230cc1c"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('readability': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":2}
