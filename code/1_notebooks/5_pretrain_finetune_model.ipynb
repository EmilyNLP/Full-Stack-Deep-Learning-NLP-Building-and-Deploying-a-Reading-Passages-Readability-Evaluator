{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Import Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "import random\n",
                "import gc\n",
                "from pathlib import Path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Constants and Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def seed_everything(seed):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "SEED = 2021 #\n",
                "BINS = [float('inf'), 1.5, 1, 0.5, 0, -0.5, -1, -1.5, -2, -2.5, -3, -3.5, float('-inf')] # map the raw score to readability level from 1 to 12(easy to hard)\n",
                "MAX_LENGTH = 256 # the maximum length of the texts feed to the model\n",
                "CORPORA_LIST = ['simplewiki','wiki','bookcorpus']\n",
                "\n",
                "TRAIN_FILE_ORIG=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"train.csv\")\n",
                "TRAIN_FILE_SPLIT=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"train_split.csv\")\n",
                "VAL_FILE_SPLIT=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"val_split.csv\")\n",
                "TRAIN_FILE_EXTENDED=os.path.join(Path(os.getcwd()).parent,'data','training','extended','train_augmented.csv')\n",
                "\n",
                "BASELINE_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"base\")\n",
                "PRETRAIN_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"pretrain\")\n",
                "FINETUNE_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"finetune\")\n",
                "FINAL_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"final\")\n",
                "\n",
                "EMBEDDINGS_DIR=os.path.join(Path(os.getcwd()).parent,'data','embeddings')\n",
                "EXTENDED_DATA_DIR=os.path.join(Path(os.getcwd()).parent,'data','training','extended')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model_name, training_file, out_dir, model_path=None, pretrain=False, val_file=None, num_epochs=3):\n",
                "    #Initialize the model and tokenizer   \n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    config = AutoConfig.from_pretrained(model_name, num_labels=1)\n",
                "    if model_path:\n",
                "        model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
                "    else:\n",
                "        model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
                "    #Load the training data, tokenize the text and covert into training dataloader\n",
                "    training_df = pd.read_csv(training_file)\n",
                "    training_embeddings=tokenizer(training_df['excerpt'].to_list(), padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
                "    training_dataset=TensorDataset(training_embeddings['input_ids'],training_embeddings['attention_mask'],torch.tensor(training_df['target'].astype('float32')))\n",
                "    training_sampler = RandomSampler(training_dataset)\n",
                "    training_dataloader = DataLoader(training_dataset,sampler=training_sampler,batch_size=8)\n",
                "    #Load the validation data(if it is provided), tokenize it and convert into validation dataloader\n",
                "    if val_file:\n",
                "        val_df = pd.read_csv(val_file)\n",
                "        val_embeddings=tokenizer(val_df['excerpt'].to_list(), padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
                "        val_dataset=TensorDataset(val_embeddings['input_ids'],val_embeddings['attention_mask'],torch.tensor(val_df['target'].astype('float32')))\n",
                "        val_sampler = SequentialSampler(val_dataset)\n",
                "        val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=8)\n",
                "    #Set up training optimizer and learning rate\n",
                "    num_training_steps = num_epochs * len(training_dataloader)\n",
                "    if pretrain: \n",
                "        lr=1e-6 # lower learning rate for pretraining\n",
                "    else:\n",
                "        lr=3e-5 # higher learning rate for fine-tuning\n",
                "    optimizer = AdamW(model.parameters(), lr=lr)\n",
                "    lr_scheduler = get_scheduler(\n",
                "        \"linear\",\n",
                "        optimizer=optimizer,\n",
                "        num_warmup_steps=0,\n",
                "        num_training_steps=num_training_steps\n",
                "    )\n",
                "    #if you have GPU on you computer, specify the device as\"\"cuda\", or else you can comment out next 2 lines\n",
                "    device = torch.device(\"cuda\")\n",
                "    model.to(device)\n",
                "    #training \n",
                "    for epoch in range(num_epochs):\n",
                "        #Set the model into train mode, then torch could do the backpropagation over the loss to update the weights of model \n",
                "        model.train()\n",
                "        print('epoch=',epoch+1)\n",
                "        epoch_iterator = tqdm(training_dataloader, desc=\"Iteration\")\n",
                "        tr_loss=0\n",
                "        for batch in epoch_iterator:\n",
                "            batch = tuple(t.to(device) for t in batch)\n",
                "            inputs = {\n",
                "                    \"input_ids\": batch[0],\n",
                "                    \"attention_mask\": batch[1],\n",
                "                    \"labels\": batch[2],\n",
                "                    }\n",
                "            outputs = model(**inputs)\n",
                "            loss = outputs.loss\n",
                "            epoch_iterator.set_description('(loss=%g)' % loss)\n",
                "            tr_loss+=loss.item()\n",
                "            loss.backward() #compute gradient of loss over parameters\n",
                "            optimizer.step() # update weights/parameters\n",
                "            lr_scheduler.step() #update learning rate\n",
                "            optimizer.zero_grad() # clear the gradient from this step\n",
                "        tr_loss/=len(training_dataloader)\n",
                "        print('tr_loss=',tr_loss)\n",
                "        #If there is validation data, evaluate the model at each epoch\n",
                "        if val_file:\n",
                "            model.eval()\n",
                "            val_loss=0\n",
                "            for batch in val_dataloader:\n",
                "                #only need evalute the model(no mini-batch Stochastic Gradient Descent needed), set the torch with no_grad to speed up \n",
                "                with torch.no_grad():\n",
                "                    batch = tuple(t.to(device) for t in batch)\n",
                "                    inputs = {\n",
                "                            \"input_ids\": batch[0],\n",
                "                            \"attention_mask\": batch[1],\n",
                "                                }\n",
                "                    labels=batch[2]\n",
                "                    outputs = model(**inputs).logits\n",
                "                    loss_fct = torch.nn.MSELoss()\n",
                "                    eval_loss = loss_fct(outputs.view(-1), labels.view(-1)).item()\n",
                "                    val_loss+=eval_loss\n",
                "            val_loss=val_loss/len(val_dataloader)\n",
                "            print('eval_loss=',val_loss)\n",
                "    #Save the model after finish the training\n",
                "    model.save_pretrained(out_dir)\n",
                "    #Clean the memory\n",
                "    del model\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect() "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(model_name, model_dir,excerpt):\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    config = AutoConfig.from_pretrained(model_dir, num_labels=1)\n",
                "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, config=config)\n",
                "    embeddings=tokenizer(excerpt, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
                "    model.eval()\n",
                "    inputs = {\"input_ids\": embeddings['input_ids'],\"attention_mask\": embeddings['attention_mask']}\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs).logits\n",
                "        score=outputs.view(-1).item()\n",
                "    return score"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Pre-train the model with the augmented dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
                        "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch= 1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(loss=0.5156): 100%|██████████| 1263/1263 [06:03<00:00,  3.47it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tr_loss= 0.493597621981553\n",
                        "eval_loss= 0.6020913695373505\n"
                    ]
                }
            ],
            "source": [
                "seed_everything(SEED)\n",
                "train(model_name=\"roberta-base\",training_file=TRAIN_FILE_EXTENDED, out_dir=PRETRAIN_MODEL_DIR, pretrain=True, val_file=TRAIN_FILE_SPLIT, num_epochs=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-tune the pretrained model with the split train dataset\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch= 1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(loss=0.0296913): 100%|██████████| 319/319 [01:45<00:00,  3.02it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tr_loss= 0.3901183171121865\n",
                        "eval_loss= 0.44263559952378273\n"
                    ]
                }
            ],
            "source": [
                "train(model_name=\"roberta-base\",training_file=TRAIN_FILE_SPLIT, out_dir=FINETUNE_MODEL_DIR, model_path=PRETRAIN_MODEL_DIR,val_file=VAL_FILE_SPLIT, num_epochs=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Make a prediction to check if the performance of the model is improved through pre-training\n",
                "\n",
                "We make prediction with the pre-trained and fine-tuned model for the same example as with baseline model. The ground truth score is -2.045, the score from baseline model is -1.42, and the sore from current model is -1.49. The new model apparently performs better than the baseline model.   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "As a statesman, it was the good fortune of Mr. Gladstone that his career was not associated with war. The reforms which he effected, the triumphs which he achieved, were not won by the supreme arbitrament of the sword. The reforms which he effected and the triumphs which he achieved were the result of his power of persuasion over his fellow-men. The reforms which he achieved in many ways amounted to a revolution. They changed, in many particulars, the face of the realm. After Sir Robert Peel had adopted the great principle which eventually carried England from protection to free trade, it was Mr. Gladstone who created the financial system which has been admitted ever since by all students of finance, as the secret of Great Britain's commercial success. He enforced the extension of the suffrage to the masses of the nation, and practically thereby made the government of monarchical England as democratic as that of any republic. \n",
                        "\n",
                        "the prediction is -1.5103332996368408, the ground truth target is -2.045048792\n"
                    ]
                }
            ],
            "source": [
                "val_df=pd.read_csv(VAL_FILE_SPLIT)\n",
                "sample_excerpt=val_df.iloc[[16]]\n",
                "text=sample_excerpt['excerpt'].to_list()[0]\n",
                "prediction=predict('roberta-base',FINETUNE_MODEL_DIR,text)\n",
                "print(text,\"\\n\")\n",
                "print(f\"the prediction is {prediction}, the ground truth target is {sample_excerpt['target'].to_list()[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Fine-tune the model from the checkpoint after pre-training with the whole original training dateset to obtain the final model\n",
                "\n",
                "From previous experiment, we have proved that pre-training the model with extended datasets indeed improved the result. At this point, we fine-tune the model from the check point after pre-training with the whole original training dataset to finalize the model for inference.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch= 1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(loss=1.09134): 100%|██████████| 355/355 [01:53<00:00,  3.13it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tr_loss= 0.3986725677172063\n"
                    ]
                }
            ],
            "source": [
                "train(model_name=\"roberta-base\",training_file=TRAIN_FILE_ORIG, out_dir=FINAL_MODEL_DIR, model_path=PRETRAIN_MODEL_DIR, num_epochs=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Make prediction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "As a statesman, it was the good fortune of Mr. Gladstone that his career was not associated with war. The reforms which he effected, the triumphs which he achieved, were not won by the supreme arbitrament of the sword. The reforms which he effected and the triumphs which he achieved were the result of his power of persuasion over his fellow-men. The reforms which he achieved in many ways amounted to a revolution. They changed, in many particulars, the face of the realm. After Sir Robert Peel had adopted the great principle which eventually carried England from protection to free trade, it was Mr. Gladstone who created the financial system which has been admitted ever since by all students of finance, as the secret of Great Britain's commercial success. He enforced the extension of the suffrage to the masses of the nation, and practically thereby made the government of monarchical England as democratic as that of any republic. \n",
                        "\n",
                        "the prediction is -1.539483904838562, the ground truth target is -2.045048792\n"
                    ]
                }
            ],
            "source": [
                "prediction=predict('roberta-base',FINAL_MODEL_DIR,text)\n",
                "print(text,\"\\n\")\n",
                "print(f\"the prediction is {prediction}, the ground truth target is {sample_excerpt['target'].to_list()[0]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "the readability score is 0.31775805354118347, the difficulty level is 4 out of 1 to 12\n"
                    ]
                }
            ],
            "source": [
                "text=\"Every person in this family must read at least 30 minutes daily.\"\n",
                "prediction=predict('roberta-base',FINAL_MODEL_DIR,text)\n",
                "print(f\"the readability score is {prediction}, the difficulty level is {np.digitize(prediction,BINS)} out of 1 to 12\" )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "the readability score is -0.7596416473388672, the difficulty level is 6 out of 1 to 12\n"
                    ]
                }
            ],
            "source": [
                "text=\"Dune is set in the distant future amidst a feudal interstellar society in which various noble houses control planetary fiefs. It tells the story of young Paul Atreides, whose family accepts the stewardship of the planet Arrakis. While the planet is an inhospitable and sparsely populated desert wasteland, it is the only source of melange, or 'spice', a drug that extends life and enhances mental abilities.\"\n",
                "prediction=predict('roberta-base',FINAL_MODEL_DIR,text)\n",
                "print(f\"the readability score is {prediction}, the difficulty level is {np.digitize(prediction,BINS)} out of 1 to 12\"  )"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1bb66c20f33883921ac4c6fb43838a8dacd3a8d29c92ea31e661ca910230cc1c"
        },
        "kernelspec": {
            "display_name": "Python 3.8.12 64-bit ('readability': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
