{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Import Dependencies\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler,TensorDataset\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "import random\n",
                "import gc\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import StratifiedKFold,StratifiedShuffleSplit"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Functions and constants"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def seed_everything(seed=2021):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "SEED = 2021 #\n",
                "BINS = [float('inf'), 1.5, 1, 0.5, 0, -0.5, -1, -1.5, -2, -2.5, -3, -3.5, float('-inf')] # map the raw score to readability level from 1 to 12(easy to hard)\n",
                "MAX_LENGTH = 256 # the maximum length of the texts feed to the model\n",
                "CORPORA_LIST = ['simplewiki','wiki','bookcorpus']\n",
                "\n",
                "\n",
                "TRAIN_FILE_ORIG=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"train.csv\")\n",
                "TRAIN_FILE_SPLIT=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"train_split.csv\")\n",
                "VAL_FILE_SPLIT=os.path.join(Path(os.getcwd()).parent,'data',\"training\",\"original\",\"val_split.csv\")\n",
                "TRAIN_FILE_EXTENDED=os.path.join(Path(os.getcwd()).parent,'data','training','extended','train_augmented.csv')\n",
                "\n",
                "BASELINE_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"base\")\n",
                "PRETRAIN_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"pretrain\")\n",
                "FINETUNE_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"finetune\")\n",
                "FINAL_MODEL_DIR=os.path.join(Path(os.getcwd()).parent,\"models\",\"final\")\n",
                "\n",
                "EMBEDDINGS_DIR=os.path.join(Path(os.getcwd()).parent,'data','embeddings')\n",
                "EXTENDED_DATA_DIR=os.path.join(Path(os.getcwd()).parent,'data','training','extended')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model_name, training_file, out_dir, model_path=None, pretrain=False, val_file=None, num_epochs=3):\n",
                "    #Initialize the model and tokenizer   \n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    config = AutoConfig.from_pretrained(model_name, num_labels=1)\n",
                "    if model_path:\n",
                "        model = AutoModelForSequenceClassification.from_pretrained(model_path, config=config)\n",
                "    else:\n",
                "        model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
                "    #Load the training data, tokenize the text and covert into training dataloader\n",
                "    training_df = pd.read_csv(training_file)\n",
                "    training_embeddings=tokenizer(training_df['excerpt'].to_list(), padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
                "    training_dataset=TensorDataset(training_embeddings['input_ids'],training_embeddings['attention_mask'],torch.tensor(training_df['target'].astype('float32')))\n",
                "    training_sampler = RandomSampler(training_dataset)\n",
                "    training_dataloader = DataLoader(training_dataset,sampler=training_sampler,batch_size=8)\n",
                "    #Load the validation data(if it is provided), tokenize it and convert into validation dataloader\n",
                "    if val_file:\n",
                "        val_df = pd.read_csv(val_file)\n",
                "        val_embeddings=tokenizer(val_df['excerpt'].to_list(), padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
                "        val_dataset=TensorDataset(val_embeddings['input_ids'],val_embeddings['attention_mask'],torch.tensor(val_df['target'].astype('float32')))\n",
                "        val_sampler = SequentialSampler(val_dataset)\n",
                "        val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=8)\n",
                "    #Set up training optimizer and learning rate\n",
                "    num_training_steps = num_epochs * len(training_dataloader)\n",
                "    if pretrain: \n",
                "        lr=1e-6 # lower learning rate for pretraining\n",
                "    else:\n",
                "        lr=3e-5 # higher learning rate for fine-tuning\n",
                "    optimizer = AdamW(model.parameters(), lr=lr)\n",
                "    lr_scheduler = get_scheduler(\n",
                "        \"linear\",\n",
                "        optimizer=optimizer,\n",
                "        num_warmup_steps=0,\n",
                "        num_training_steps=num_training_steps\n",
                "    )\n",
                "    #if you have GPU on you computer, specify the device as\"\"cuda\", or else you can comment out next 2 lines\n",
                "    device = torch.device(\"cuda\")\n",
                "    model.to(device)\n",
                "    #training \n",
                "    for epoch in range(num_epochs):\n",
                "        #Set the model into train mode, then torch could do the backpropagation over the loss to update the weights of model \n",
                "        model.train()\n",
                "        print('epoch=',epoch+1)\n",
                "        epoch_iterator = tqdm(training_dataloader, desc=\"Iteration\")\n",
                "        tr_loss=0\n",
                "        for batch in epoch_iterator:\n",
                "            batch = tuple(t.to(device) for t in batch)\n",
                "            inputs = {\n",
                "                    \"input_ids\": batch[0],\n",
                "                    \"attention_mask\": batch[1],\n",
                "                    \"labels\": batch[2],\n",
                "                    }\n",
                "            outputs = model(**inputs)\n",
                "            loss = outputs.loss\n",
                "            epoch_iterator.set_description('(loss=%g)' % loss)\n",
                "            tr_loss+=loss.item()\n",
                "            loss.backward() #compute gradient of loss over parameters\n",
                "            optimizer.step() # update weights/parameters\n",
                "            lr_scheduler.step() #update learning rate\n",
                "            optimizer.zero_grad() # clear the gradient from this step\n",
                "        tr_loss/=len(training_dataloader)\n",
                "        print('tr_loss=',tr_loss)\n",
                "        #If there is validation data, evaluate the model at each epoch\n",
                "        if val_file:\n",
                "            model.eval()\n",
                "            val_loss=0\n",
                "            for batch in val_dataloader:\n",
                "                #only need evalute the model(no mini-batch Stochastic Gradient Descent needed), set the torch with no_grad to speed up \n",
                "                with torch.no_grad():\n",
                "                    batch = tuple(t.to(device) for t in batch)\n",
                "                    inputs = {\n",
                "                            \"input_ids\": batch[0],\n",
                "                            \"attention_mask\": batch[1],\n",
                "                                }\n",
                "                    labels=batch[2]\n",
                "                    outputs = model(**inputs).logits\n",
                "                    loss_fct = torch.nn.MSELoss()\n",
                "                    eval_loss = loss_fct(outputs.view(-1), labels.view(-1)).item()\n",
                "                    val_loss+=eval_loss\n",
                "            val_loss=val_loss/len(val_dataloader)\n",
                "            print('eval_loss=',val_loss)\n",
                "    #Save the model after finish the training\n",
                "    model.save_pretrained(out_dir)\n",
                "    #Clean the memory\n",
                "    del model\n",
                "    torch.cuda.empty_cache()\n",
                "    gc.collect() "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(model_name, model_dir,excerpt):\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "    config = AutoConfig.from_pretrained(model_dir, num_labels=1)\n",
                "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, config=config)\n",
                "    embeddings=tokenizer(excerpt, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
                "    model.eval()\n",
                "    inputs = {\"input_ids\": embeddings['input_ids'],\"attention_mask\": embeddings['attention_mask']}\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs).logits\n",
                "        score=outputs.view(-1).item()\n",
                "    return score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "#split the original training dataset into train and validate dataset\n",
                "seed_everything(seed=SEED)\n",
                "BINS = [float('inf'), 1.5, 1, 0.5, 0, -0.5, -1, -1.5, -2, -2.5, -3, -3.5, float('-inf')] # map the raw score to readability level from 1 to 12(easy to hard)\n",
                "train_df_orig=pd.read_csv(TRAIN_FILE_ORIG)\n",
                "train_df_orig['level']=train_df_orig['target'].apply(lambda x: np.digitize(x,BINS))\n",
                "skf = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=67)\n",
                "splits=skf.split(train_df_orig,train_df_orig.level)\n",
                "for train_idx, test_idx in splits:\n",
                "    train_df=train_df_orig.loc[train_idx]\n",
                "    val_df=train_df_orig.loc[test_idx]\n",
                "train_df.reset_index(inplace=True,drop=True)\n",
                "val_df.reset_index(inplace=True,drop=True)\n",
                "train_df.to_csv(TRAIN_FILE_SPLIT)\n",
                "val_df.to_csv(VAL_FILE_SPLIT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
                        "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
                        "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "epoch= 1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(loss=0.335987): 100%|██████████| 319/319 [01:40<00:00,  3.18it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tr_loss= 0.5338854886427942\n",
                        "eval_loss= 0.3738563994152678\n",
                        "epoch= 2\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(loss=0.0996832): 100%|██████████| 319/319 [01:31<00:00,  3.48it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tr_loss= 0.23649449868355424\n",
                        "eval_loss= 0.3873924497101042\n",
                        "epoch= 3\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "(loss=0.269642): 100%|██████████| 319/319 [01:30<00:00,  3.52it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tr_loss= 0.13304433722755518\n",
                        "eval_loss= 0.32840617270105416\n"
                    ]
                }
            ],
            "source": [
                "train(model_name=\"roberta-base\",training_file=TRAIN_FILE_SPLIT, out_dir=BASELINE_MODEL_DIR, val_file=VAL_FILE_SPLIT, num_epochs=3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "As a statesman, it was the good fortune of Mr. Gladstone that his career was not associated with war. The reforms which he effected, the triumphs which he achieved, were not won by the supreme arbitrament of the sword. The reforms which he effected and the triumphs which he achieved were the result of his power of persuasion over his fellow-men. The reforms which he achieved in many ways amounted to a revolution. They changed, in many particulars, the face of the realm. After Sir Robert Peel had adopted the great principle which eventually carried England from protection to free trade, it was Mr. Gladstone who created the financial system which has been admitted ever since by all students of finance, as the secret of Great Britain's commercial success. He enforced the extension of the suffrage to the masses of the nation, and practically thereby made the government of monarchical England as democratic as that of any republic. \n",
                        "\n",
                        "the prediction is -1.4239850044250488, the ground truth target is -2.045048792\n"
                    ]
                }
            ],
            "source": [
                "val_df=pd.read_csv(VAL_FILE_SPLIT)\n",
                "sample_excerpt=val_df.iloc[[16]]\n",
                "text=sample_excerpt['excerpt'].to_list()[0]\n",
                "prediction=predict('roberta-base',BASELINE_MODEL_DIR,text)\n",
                "print(text,\"\\n\")\n",
                "print(f\"the prediction is {prediction}, the ground truth target is {sample_excerpt['target'].to_list()[0]}\")"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1bb66c20f33883921ac4c6fb43838a8dacd3a8d29c92ea31e661ca910230cc1c"
        },
        "kernelspec": {
            "display_name": "Python 3.8.12 64-bit ('readability': conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
